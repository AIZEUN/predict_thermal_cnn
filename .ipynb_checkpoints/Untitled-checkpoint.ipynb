{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e3d6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head  파일 길이 :  9\n",
      "head  :  ./train_src/head\\head_2018-01-31-16-17-50_00001.bmp\n",
      "noise  파일 길이 :  14\n",
      "noise  :  ./train_src/noise\\noise_2018-01-30-22-08-04_00001.bmp\n",
      "finger  파일 길이 :  11\n",
      "finger  :  ./train_src/finger\\finger_2018-01-30-09-24-37_00029.bmp\n",
      "ok 34\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"./train_src\"\n",
    "categories = [\"head\", \"noise\", \"finger\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.bmp\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./numpy_data/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a471e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 64, 64, 3)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./numpy_data/multi_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39b4bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"head\", \"noise\", \"finger\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a476da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1085734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 4,214,723\n",
      "Trainable params: 4,214,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae14852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1168 - accuracy: 0.3600\n",
      "Epoch 00001: val_loss improved from inf to 2.73479, saving model to ./model\\multi_img_classification.model\n",
      "WARNING:tensorflow:From C:\\Users\\ict\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\ict\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1168 - accuracy: 0.3600 - val_loss: 2.7348 - val_accuracy: 0.2222\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2693 - accuracy: 0.4800\n",
      "Epoch 00002: val_loss improved from 2.73479 to 1.37118, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2693 - accuracy: 0.4800 - val_loss: 1.3712 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3864 - accuracy: 0.3600\n",
      "Epoch 00003: val_loss improved from 1.37118 to 0.97009, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3864 - accuracy: 0.3600 - val_loss: 0.9701 - val_accuracy: 0.4444\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1218 - accuracy: 0.4800\n",
      "Epoch 00004: val_loss did not improve from 0.97009\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1218 - accuracy: 0.4800 - val_loss: 0.9953 - val_accuracy: 0.5556\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0132 - accuracy: 0.4400\n",
      "Epoch 00005: val_loss did not improve from 0.97009\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0132 - accuracy: 0.4400 - val_loss: 1.0742 - val_accuracy: 0.2222\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0458 - accuracy: 0.4800\n",
      "Epoch 00006: val_loss did not improve from 0.97009\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0458 - accuracy: 0.4800 - val_loss: 1.1120 - val_accuracy: 0.2222\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8143 - accuracy: 0.6000\n",
      "Epoch 00007: val_loss did not improve from 0.97009\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8143 - accuracy: 0.6000 - val_loss: 1.0736 - val_accuracy: 0.4444\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6536 - accuracy: 0.7200\n",
      "Epoch 00008: val_loss did not improve from 0.97009\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6536 - accuracy: 0.7200 - val_loss: 1.0120 - val_accuracy: 0.5556\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.8000\n",
      "Epoch 00009: val_loss improved from 0.97009 to 0.96790, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6446 - accuracy: 0.8000 - val_loss: 0.9679 - val_accuracy: 0.4444\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7154 - accuracy: 0.7200\n",
      "Epoch 00010: val_loss improved from 0.96790 to 0.92801, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7154 - accuracy: 0.7200 - val_loss: 0.9280 - val_accuracy: 0.4444\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.6800\n",
      "Epoch 00011: val_loss improved from 0.92801 to 0.88456, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5720 - accuracy: 0.6800 - val_loss: 0.8846 - val_accuracy: 0.4444\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5106 - accuracy: 0.8000\n",
      "Epoch 00012: val_loss improved from 0.88456 to 0.83702, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5106 - accuracy: 0.8000 - val_loss: 0.8370 - val_accuracy: 0.6667\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8400\n",
      "Epoch 00013: val_loss improved from 0.83702 to 0.78775, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4277 - accuracy: 0.8400 - val_loss: 0.7877 - val_accuracy: 0.8889\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4260 - accuracy: 0.8400\n",
      "Epoch 00014: val_loss improved from 0.78775 to 0.73714, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4260 - accuracy: 0.8400 - val_loss: 0.7371 - val_accuracy: 0.8889\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8800\n",
      "Epoch 00015: val_loss improved from 0.73714 to 0.67866, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3355 - accuracy: 0.8800 - val_loss: 0.6787 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9200\n",
      "Epoch 00016: val_loss improved from 0.67866 to 0.61386, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2804 - accuracy: 0.9200 - val_loss: 0.6139 - val_accuracy: 0.8889\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9600\n",
      "Epoch 00017: val_loss improved from 0.61386 to 0.54864, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2506 - accuracy: 0.9600 - val_loss: 0.5486 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss improved from 0.54864 to 0.48416, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1940 - accuracy: 1.0000 - val_loss: 0.4842 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1818 - accuracy: 1.0000\n",
      "Epoch 00019: val_loss improved from 0.48416 to 0.40659, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1818 - accuracy: 1.0000 - val_loss: 0.4066 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss improved from 0.40659 to 0.33144, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1500 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss improved from 0.33144 to 0.28308, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1152 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.8889\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9600\n",
      "Epoch 00022: val_loss improved from 0.28308 to 0.23775, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1058 - accuracy: 0.9600 - val_loss: 0.2377 - val_accuracy: 0.8889\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9600\n",
      "Epoch 00023: val_loss improved from 0.23775 to 0.18924, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0763 - accuracy: 0.9600 - val_loss: 0.1892 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss improved from 0.18924 to 0.16076, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.1608 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9600\n",
      "Epoch 00025: val_loss improved from 0.16076 to 0.15771, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0594 - accuracy: 0.9600 - val_loss: 0.1577 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss improved from 0.15771 to 0.14450, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss improved from 0.14450 to 0.09297, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.0930 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss improved from 0.09297 to 0.06749, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss improved from 0.06749 to 0.06592, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss improved from 0.06592 to 0.06547, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9600\n",
      "Epoch 00031: val_loss improved from 0.06547 to 0.04520, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0488 - accuracy: 0.9600 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 00032: val_loss improved from 0.04520 to 0.03065, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 00033: val_loss improved from 0.03065 to 0.02799, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 00034: val_loss did not improve from 0.02799\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 00035: val_loss did not improve from 0.02799\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 00036: val_loss did not improve from 0.02799\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00037: val_loss did not improve from 0.02799\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 00038: val_loss did not improve from 0.02799\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 00039: val_loss did not improve from 0.02799\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28980267",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'batch_outputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-84a92ba86cc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model/multi_img_classification.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"{0:0.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1612\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m     \u001b[0mall_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'batch_outputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "caltech_dir = \"./multi_img_data/imgs_others_test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "model = load_model('./model/multi_img_classification.model')\n",
    "\n",
    "prediction = model.predict(X)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"head\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"noise\"\n",
    "    else: pre_ans_str = \"finger\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36e7d47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.16.1\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: None\n",
      "License: BSD\n",
      "Location: c:\\users\\ict\\anaconda3\\envs\\tensorflow\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: tifffile, tensorflow, tensorflow-hub, tensorboard, scipy, scikit-learn, scikit-image, PyWavelets, pandas, opt-einsum, matplotlib, Keras, Keras-Preprocessing, Keras-Applications, imageio, h5py\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
